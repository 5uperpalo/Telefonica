{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 Initialization\n",
    "## Section 1.1 Copy results of UK_HomeDetection_IMC20 to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -copyFromLocal datasets/012020/home_ldn.csv QoE/London/London_Jan_2020/\n",
    "hdfs dfs -copyFromLocal datasets/012020/home_birm.csv QoE/Birmingham/Birmingham_Jan_2020/\n",
    "hdfs dfs -copyFromLocal datasets/012020/home_lpool.csv QoE/Liverpool/Liverpool_Jan_2020/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2 PySpark and Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_HOME\"] = '/usr/local/spark/spark-1.6.2-bin-hadoop2.6'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--master local[*] --deploy-mode client --packages com.databricks:spark-csv_2.11:1.3.0 pyspark-shell\"\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import sql\n",
    "from pyspark.sql.types import DoubleType, StructType, StructField, StringType\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql.functions import col, lit, count, sum, avg\n",
    "\n",
    "print('starting')\n",
    "conf = SparkConf().setAppName('AntennaConnectionDensity')#.setMaster('local[*]')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# we need HiveContext to use Hive builtin functions:\n",
    "# hive builtin functions : https://support.treasuredata.com/hc/en-us/articles/360001457367-Hive-Built-in-Aggregate-Functions\n",
    "sqlContext = HiveContext(sc)\n",
    "\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2 Define datasets we are going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dirs = ['QoE/Liverpool/Liverpool_Jan_2020/',\n",
    "             'QoE/London/London_Jan_2020/',\n",
    "             'QoE/Birmingham/Birmingham_Jan_2020/']\n",
    "\n",
    "output_files = ['Liverpool_Jan_2020',\n",
    "                'London_Jan_2020',\n",
    "                'Birmingham_Jan_2020']\n",
    "\n",
    "home_antenna_files = ['home_lpool.csv',\n",
    "                     'home_ldn.csv',\n",
    "                     'home_birm.csv']\n",
    "\n",
    "days = ['01', '02', '03']\n",
    "\n",
    "\n",
    "gyration_schema = StructType([StructField('device_id', StringType(), True),\n",
    "                              StructField('gyration', DoubleType(), True),\n",
    "                              StructField('dt', StringType(), True)])\n",
    "data_schema = StructType([StructField('device_id', StringType(), True),\n",
    "                          StructField('antenna_id', StringType(), True),\n",
    "                          StructField('time_spent', DoubleType(), True),\n",
    "                          StructField('dt', StringType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 Aggregated User Experience and Antenna Connection Density by day\n",
    "## Section 2.1 Data gathering and transformation\n",
    "\n",
    "1.     iterate over days \n",
    "2.     split each row on \"tab\" \n",
    "3.     create data frames  \n",
    "3.1    dataframes have rows where each rows begins with device_id, gyration, 2 mistery values and [antena_id(lkey), time_spent]  pairs   \n",
    "4.     join dataframes with previously identified residents and their home antennas \n",
    "5.     filter resident/nonresident datasets \n",
    "6.     group and calculate metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_dir,output_file,home_antenna in zip(data_dirs,output_files,home_antenna_files):\n",
    "    home_antenna_df = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(data_dir + home_antenna)\n",
    "    gyration_df = sqlContext.createDataFrame(sc.emptyRDD(), gyration_schema)\n",
    "    data_df = sqlContext.createDataFrame(sc.emptyRDD(), data_schema)    \n",
    "    for day in days:\n",
    "        data_temp = sc.textFile(data_dir + day + '/hour=*/part-*').map(lambda x: x.split('\\t'))\n",
    "        \n",
    "        gyration_temp_df = data_temp.map(lambda x: [x[i] for i in [0,1]])\\\n",
    "                                    .toDF(('device_id', 'gyration'))\\\n",
    "                                    .withColumn('dt', lit(day))\n",
    "        gyration_df = gyration_df.unionAll(gyration_temp_df)\n",
    "\n",
    "        data_temp_df = data_temp.flatMap(lambda x: [(x[0], x[i], x[i+1]) for i in range(5,len(x),2)])\\\n",
    "                                .toDF(('device_id', 'antenna_id', 'time_spent'))\\\n",
    "                                .withColumn('dt', lit(day))\n",
    "        data_df = data_df.unionAll(data_temp_df)\n",
    "    \n",
    "    # it is not possible to rename multiple columns with \"withColumnRenamed\"\n",
    "    # the join keeps all columns - even those in join condition -> change column names that are similar\n",
    "    home_antenna_df = home_antenna_df.drop('geometry')\n",
    "    home_antenna_df = home_antenna_df.withColumnRenamed('device_id', 'device_idd')\n",
    "    home_antenna_df = home_antenna_df.withColumnRenamed('antenna_id', 'home_antenna_id')\n",
    "\n",
    "    # 'left' join will create null cells in \"home_antenna_id\" column for non_residents\n",
    "    gyration_df = gyration_df.join(home_antenna_df,[home_antenna_df['device_idd']==gyration_df['device_id']], 'left').drop('device_idd')\n",
    "    \n",
    "    data_df = data_df.join(home_antenna_df,[home_antenna_df['device_idd']==data_df['device_id'], home_antenna_df['home_antenna_id']==data_df['antenna_id']], 'left').drop('device_idd')\n",
    "    \n",
    "    # divide the dataset on resident/nonresidents\n",
    "    gyration_df = gyration_df.filter(col('home_antenna_id').isNotNull())\n",
    "    \n",
    "    data_df_residents = data_df.filter(col('home_antenna_id').isNotNull()).drop('home_antenna_id')\n",
    "    data_df_nonresidents = data_df.filter(col('home_antenna_id').isNull()).drop('home_antenna_id')\n",
    "\n",
    "    \n",
    "    # this step can be easily changed to compute same metrics only for particular days - e.g. weekends/weekdays\n",
    "    # by adding filter statement to 2nd line : .filter(col('dt').isin(['01','02', ...]))\n",
    "    # [NOTE] - there can be only 1 groupby statement -> no chaining of grouby, it must be a new command\n",
    "    gyration_df = gyration_df.groupby('device_id','home_antenna_id','dt')\\\n",
    "                             .agg(sum('gyration').alias('sum_gyration'))\n",
    "    gyration_df = gyration_df.groupby('device_id','home_antenna_id')\\\n",
    "                             .agg(avg('sum_gyration').alias('avg_sum_gyration'))\n",
    "    gyration_df.coalesce(1).write.mode('append').format('com.databricks.spark.csv').option('header', 'true').save(data_dir + 'Gyration_' + output_file + '_residents')\n",
    "    #gyration_df.coalesce(1).write.format('com.databricks.spark.csv').option('header', 'true').csv(data_dir + 'Gyration_' + output_file + '_residents')\n",
    "    \n",
    "    data_df_residents = data_df_residents.groupby('antenna_id','dt')\\\n",
    "                                         .agg(sum('time_spent').alias('sum_time'), count('device_id').alias('count_device_id'))\n",
    "    data_df_residents = data_df_residents.groupby('antenna_id')\\\n",
    "                                         .agg(avg('sum_time').alias('avg_sum_time'), avg('count_device_id').alias('avg_count_device_id'))\n",
    "    data_df_residents.coalesce(1).write.mode('append').format('com.databricks.spark.csv').option('header', 'true').save(data_dir + 'AntennaConnectionDensity_' + output_file + '_residents')\n",
    "                                         \n",
    "    data_df_nonresidents = data_df_nonresidents.groupby('antenna_id','dt')\\\n",
    "                                               .agg(sum('time_spent').alias('sum_time'), count('device_id').alias('count_device_id'))\n",
    "    data_df_nonresidents = data_df_nonresidents.groupby('antenna_id')\\\n",
    "                                               .agg(avg('sum_time').alias('avg_sum_time'), avg('count_device_id').alias('avg_count_device_id'))\n",
    "    data_df_nonresidents.coalesce(1).write.mode('append').format('com.databricks.spark.csv').option('header', 'true').save(data_dir + 'AntennaConnectionDensity_' + output_file + '_nonresidents')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
