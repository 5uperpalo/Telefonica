{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 Initialization\n",
    "## Section 1.1 Copy results from \"UK_HomeDetection_IMC20\" and \"IMC2020_AntennaIMDAssignment\" to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -copyFromLocal datasets/Telefonica_Antenna/XG/telefonica_antenna_london_imd_pd.csv QoE/London/\n",
    "hdfs dfs -copyFromLocal datasets/Telefonica_Antenna/XG/telefonica_antenna_birmingham_imd_pd.csv QoE/Birmingham/\n",
    "hdfs dfs -copyFromLocal datasets/Telefonica_Antenna/XG/telefonica_antenna_liverpool_imd_pd.csv QoE/Liverpool/\n",
    "\n",
    "hdfs dfs -copyFromLocal datasets/012020/home_ldn.csv QoE/London/London_Jan_2020/\n",
    "hdfs dfs -copyFromLocal datasets/012020/home_birm.csv QoE/Birmingham/Birmingham_Jan_2020/\n",
    "hdfs dfs -copyFromLocal datasets/012020/home_lpool.csv QoE/Liverpool/Liverpool_Jan_2020/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2 PySpark and Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_HOME\"] = '/usr/local/spark/spark-1.6.2-bin-hadoop2.6'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--master local[*] --deploy-mode client --packages com.databricks:spark-csv_2.11:1.3.0 pyspark-shell\"\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark import sql\n",
    "from pyspark.sql import HiveContext, Window, SQLContext\n",
    "from pyspark.sql.types import DoubleType, StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col, lit, count, sum, avg, max, array\n",
    "\n",
    "print('starting')\n",
    "conf = SparkConf().setAppName('ML_datasets')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# we need HiveContext to use Hive builtin functions:\n",
    "# hive builtin functions : https://support.treasuredata.com/hc/en-us/articles/360001457367-Hive-Built-in-Aggregate-Functions\n",
    "sqlContext = HiveContext(sc)\n",
    "\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2 Define datasets we are going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dirs = ['QoE/Liverpool/Liverpool_Jan_2020/',\n",
    "             'QoE/London/London_Jan_2020/',\n",
    "             'QoE/Birmingham/Birmingham_Jan_2020/']\n",
    "\n",
    "output_files = ['lpool_012020',\n",
    "                'lndn_012020',\n",
    "                'birm_012020']\n",
    "\n",
    "home_antenna_files = ['home_lpool.csv',\n",
    "                     'home_ldn.csv',\n",
    "                     'home_birm.csv']\n",
    "\n",
    "antenna_info_files = ['QoE/Liverpool/telefonica_antenna_liverpool_imd_pd.csv',\n",
    "                      'QoE/London/telefonica_antenna_london_imd_pd.csv',\n",
    "                      'QoE/Birmingham/telefonica_antenna_birmingham_imd_pd.csv']\n",
    "\n",
    "days = ['01', '02', '03']\n",
    "\n",
    "data_schema = StructType([StructField('device_id', StringType(), True),\n",
    "                          StructField('antenna_id', StringType(), True),\n",
    "                          StructField('time_spent', DoubleType(), True),\n",
    "                          StructField('dt', StringType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobility/CDR features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_dir,output_file,home_antenna,antenna_info_file in zip(data_dirs[:1],output_files[:1],home_antenna_files[:1],antenna_info_files[:1]):\n",
    "    home_antenna_df = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(data_dir + home_antenna)\n",
    "    data_df = sqlContext.createDataFrame(sc.emptyRDD(), data_schema)    \n",
    "    for day in days:\n",
    "        data_temp = sc.textFile(data_dir + day + '/hour=*/part-*').map(lambda x: x.split('\\t'))\n",
    "        data_temp_df = data_temp.flatMap(lambda x: [(x[0], x[i], x[i+1]) for i in range(5,len(x),2)])\\\n",
    "                                .toDF(('device_id', 'antenna_id', 'time_spent'))\\\n",
    "                                .withColumn('dt', lit(day))\n",
    "        data_df = data_df.unionAll(data_temp_df)\n",
    "        \n",
    "    home_antenna_df = home_antenna_df.withColumnRenamed('antenna_id', 'antenna_idd')\n",
    "    home_antenna_df = home_antenna_df.drop('geometry')\n",
    "\n",
    "    antenna_info = sqlContext.read.format('com.databricks.spark.csv').option('header', 'true').option('inferSchema', 'true').load(antenna_info_file)\n",
    "    antenna_info = antenna_info.drop('geometry_voronoi')\n",
    "    antenna_info = antenna_info.drop('geometry')\n",
    "    antenna_info = antenna_info.withColumnRenamed('lkey', 'antenna_id')\n",
    "    \n",
    "    ResidentSumGenTimeSpent_df = data_df.join(home_antenna_df,'device_id')\\\n",
    "                                        .join(antenna_info, 'antenna_id')\\\n",
    "                                        .groupby('device_id','dt')\\\n",
    "                                        .pivot('generation')\\\n",
    "                                        .agg(sum('time_spent').alias('sum(time_spent)'))\\\n",
    "                                        .fillna(0)    \n",
    "    ResidentSumGenTimeSpent_df = ResidentSumGenTimeSpent_df.groupby('device_id').mean()\n",
    "    \n",
    "    ResidentSumGenImdTimeSpent_df = data_df.join(home_antenna_df,'device_id')\\\n",
    "                                          .join(antenna_info, 'antenna_id')\\\n",
    "                                          .groupby('device_id','dt')\\\n",
    "                                          .pivot('IMDDecil')\\\n",
    "                                          .agg(sum('time_spent'))\\\n",
    "                                          .fillna(0)\n",
    "    ResidentSumGenImdTimeSpent_df = ResidentSumGenImdTimeSpent_df.groupby('device_id').mean()\\\n",
    "                                                                 .join(ResidentSumGenTimeSpent_df,'device_id')\n",
    "\n",
    "    AntennatSumImdTimeSpentCntDev_df = data_df.join(home_antenna_df,'device_id')\\\n",
    "                                              .join(antenna_info, 'antenna_id')\\\n",
    "                                              .groupby('antenna_id','dt')\\\n",
    "                                              .pivot('IMDDecil')\\\n",
    "                                              .agg(sum('time_spent'),count('device_id'))\\\n",
    "                                              .fillna(0)\n",
    "    AntennatSumImdTimeSpentCntDev_df = AntennatSumImdTimeSpentCntDev_df.groupby('antenna_id').mean()\n",
    "\n",
    "    ResidentSumGenImdTimeSpent_df.coalesce(1).write.mode('append').format('com.databricks.spark.csv').option('header', 'true').save(data_dir + output_file + '_' + 'ResidentSumGenImdTimeSpent')\n",
    "    AntennatSumImdTimeSpentCntDev_df.coalesce(1).write.mode('append').format('com.databricks.spark.csv').option('header', 'true').save(data_dir + output_file + '_' + 'AntennatSumImdTimeSpentCntDev')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
